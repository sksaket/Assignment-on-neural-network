{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Forest fires dataset.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNN/zUQMWp5PDPwT/LvJfPH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sksaket/Assignment-on-neural-network/blob/main/Forest_fires_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlfxWAKEeqPR",
        "outputId": "505ac636-9c71-436b-afbb-6ef57cf88732"
      },
      "source": [
        "!pip install keras\n",
        "!pip install tensorflow\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.10.0.2)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (12.0.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.42.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.13.3)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.22.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqnqHOyYfWyb"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "2fQCuQPsfW07",
        "outputId": "57aeb4b0-69ac-47c4-f96b-107261a3bf87"
      },
      "source": [
        "df=pd.read_csv('/content/forestfires.csv')\n",
        "df"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>FFMC</th>\n",
              "      <th>DMC</th>\n",
              "      <th>DC</th>\n",
              "      <th>ISI</th>\n",
              "      <th>temp</th>\n",
              "      <th>RH</th>\n",
              "      <th>wind</th>\n",
              "      <th>rain</th>\n",
              "      <th>area</th>\n",
              "      <th>dayfri</th>\n",
              "      <th>daymon</th>\n",
              "      <th>daysat</th>\n",
              "      <th>daysun</th>\n",
              "      <th>daythu</th>\n",
              "      <th>daytue</th>\n",
              "      <th>daywed</th>\n",
              "      <th>monthapr</th>\n",
              "      <th>monthaug</th>\n",
              "      <th>monthdec</th>\n",
              "      <th>monthfeb</th>\n",
              "      <th>monthjan</th>\n",
              "      <th>monthjul</th>\n",
              "      <th>monthjun</th>\n",
              "      <th>monthmar</th>\n",
              "      <th>monthmay</th>\n",
              "      <th>monthnov</th>\n",
              "      <th>monthoct</th>\n",
              "      <th>monthsep</th>\n",
              "      <th>size_category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>mar</td>\n",
              "      <td>fri</td>\n",
              "      <td>86.2</td>\n",
              "      <td>26.2</td>\n",
              "      <td>94.3</td>\n",
              "      <td>5.1</td>\n",
              "      <td>8.2</td>\n",
              "      <td>51</td>\n",
              "      <td>6.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>oct</td>\n",
              "      <td>tue</td>\n",
              "      <td>90.6</td>\n",
              "      <td>35.4</td>\n",
              "      <td>669.1</td>\n",
              "      <td>6.7</td>\n",
              "      <td>18.0</td>\n",
              "      <td>33</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>oct</td>\n",
              "      <td>sat</td>\n",
              "      <td>90.6</td>\n",
              "      <td>43.7</td>\n",
              "      <td>686.9</td>\n",
              "      <td>6.7</td>\n",
              "      <td>14.6</td>\n",
              "      <td>33</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mar</td>\n",
              "      <td>fri</td>\n",
              "      <td>91.7</td>\n",
              "      <td>33.3</td>\n",
              "      <td>77.5</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.3</td>\n",
              "      <td>97</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>mar</td>\n",
              "      <td>sun</td>\n",
              "      <td>89.3</td>\n",
              "      <td>51.3</td>\n",
              "      <td>102.2</td>\n",
              "      <td>9.6</td>\n",
              "      <td>11.4</td>\n",
              "      <td>99</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>512</th>\n",
              "      <td>aug</td>\n",
              "      <td>sun</td>\n",
              "      <td>81.6</td>\n",
              "      <td>56.7</td>\n",
              "      <td>665.6</td>\n",
              "      <td>1.9</td>\n",
              "      <td>27.8</td>\n",
              "      <td>32</td>\n",
              "      <td>2.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.44</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>large</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>513</th>\n",
              "      <td>aug</td>\n",
              "      <td>sun</td>\n",
              "      <td>81.6</td>\n",
              "      <td>56.7</td>\n",
              "      <td>665.6</td>\n",
              "      <td>1.9</td>\n",
              "      <td>21.9</td>\n",
              "      <td>71</td>\n",
              "      <td>5.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>54.29</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>large</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>514</th>\n",
              "      <td>aug</td>\n",
              "      <td>sun</td>\n",
              "      <td>81.6</td>\n",
              "      <td>56.7</td>\n",
              "      <td>665.6</td>\n",
              "      <td>1.9</td>\n",
              "      <td>21.2</td>\n",
              "      <td>70</td>\n",
              "      <td>6.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>large</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515</th>\n",
              "      <td>aug</td>\n",
              "      <td>sat</td>\n",
              "      <td>94.4</td>\n",
              "      <td>146.0</td>\n",
              "      <td>614.7</td>\n",
              "      <td>11.3</td>\n",
              "      <td>25.6</td>\n",
              "      <td>42</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>516</th>\n",
              "      <td>nov</td>\n",
              "      <td>tue</td>\n",
              "      <td>79.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>106.7</td>\n",
              "      <td>1.1</td>\n",
              "      <td>11.8</td>\n",
              "      <td>31</td>\n",
              "      <td>4.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>small</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>517 rows × 31 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    month  day  FFMC    DMC  ...  monthnov  monthoct  monthsep  size_category\n",
              "0     mar  fri  86.2   26.2  ...         0         0         0          small\n",
              "1     oct  tue  90.6   35.4  ...         0         1         0          small\n",
              "2     oct  sat  90.6   43.7  ...         0         1         0          small\n",
              "3     mar  fri  91.7   33.3  ...         0         0         0          small\n",
              "4     mar  sun  89.3   51.3  ...         0         0         0          small\n",
              "..    ...  ...   ...    ...  ...       ...       ...       ...            ...\n",
              "512   aug  sun  81.6   56.7  ...         0         0         0          large\n",
              "513   aug  sun  81.6   56.7  ...         0         0         0          large\n",
              "514   aug  sun  81.6   56.7  ...         0         0         0          large\n",
              "515   aug  sat  94.4  146.0  ...         0         0         0          small\n",
              "516   nov  tue  79.5    3.0  ...         1         0         0          small\n",
              "\n",
              "[517 rows x 31 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wqq5julSfW3J",
        "outputId": "9f018d2f-ac7f-4adb-efcc-70b0568b8314"
      },
      "source": [
        "#scaling the numerical data( leaving the target variable )\n",
        "df1=df.iloc[:,2:30]\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc=StandardScaler()\n",
        "df_norm=sc.fit_transform(df1)\n",
        "df_norm"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-8.05959472e-01, -1.32332557e+00, -1.83047676e+00, ...,\n",
              "        -4.40225453e-02, -1.72859706e-01, -7.06081245e-01],\n",
              "       [-8.10203395e-03, -1.17954077e+00,  4.88890915e-01, ...,\n",
              "        -4.40225453e-02,  5.78503817e+00, -7.06081245e-01],\n",
              "       [-8.10203395e-03, -1.04982188e+00,  5.60715454e-01, ...,\n",
              "        -4.40225453e-02,  5.78503817e+00, -7.06081245e-01],\n",
              "       ...,\n",
              "       [-1.64008316e+00, -8.46647711e-01,  4.74768113e-01, ...,\n",
              "        -4.40225453e-02, -1.72859706e-01, -7.06081245e-01],\n",
              "       [ 6.80956663e-01,  5.49002541e-01,  2.69382214e-01, ...,\n",
              "        -4.40225453e-02, -1.72859706e-01, -7.06081245e-01],\n",
              "       [-2.02087875e+00, -1.68591332e+00, -1.78044169e+00, ...,\n",
              "         2.27156334e+01, -1.72859706e-01, -7.06081245e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGLv4GAmfgTu"
      },
      "source": [
        "**PCA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsHkImj_fW7_",
        "outputId": "e7ef8562-538c-4618-92d6-96bde089ac0f"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca=PCA(n_components=28)\n",
        "pca_values=pca.fit_transform(df_norm)\n",
        "pca_values"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3.76670947e+00, -1.32025451e+00, -8.43971398e-01, ...,\n",
              "        -6.53345819e-02, -6.05082538e-15, -1.58743875e-16],\n",
              "       [ 3.90786263e-01,  8.31061522e-01, -1.10136513e+00, ...,\n",
              "         3.42618601e-02, -2.67236885e-15, -6.92610536e-16],\n",
              "       [ 6.90415596e-01,  1.17774562e+00, -1.22199841e+00, ...,\n",
              "         2.63235187e-02,  5.92028990e-15,  8.36530871e-16],\n",
              "       ...,\n",
              "       [ 9.21634000e-01, -2.64543072e-01,  2.71921606e+00, ...,\n",
              "        -2.97865814e-01, -6.98934052e-16,  4.03200598e-18],\n",
              "       [-1.62054896e+00, -9.78838231e-01,  3.31987355e-01, ...,\n",
              "         3.91949863e-02,  5.57925976e-16, -3.39227990e-17],\n",
              "       [ 4.07590654e+00, -3.67440726e-01, -2.47151775e-01, ...,\n",
              "        -2.50420726e-02,  6.17289277e-17, -8.31075187e-17]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0WtMWK4fW-V",
        "outputId": "4386d6d9-cd6c-4f47-e54c-e785d64ee472"
      },
      "source": [
        "var=pca.explained_variance_ratio_\n",
        "var"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.35522746e-01, 6.85788793e-02, 6.23572652e-02, 5.32713255e-02,\n",
              "       4.75942360e-02, 4.68009902e-02, 4.37490015e-02, 4.28025164e-02,\n",
              "       4.08875728e-02, 4.01633268e-02, 3.92926854e-02, 3.83232321e-02,\n",
              "       3.64221503e-02, 3.63217289e-02, 3.57856782e-02, 3.50087806e-02,\n",
              "       3.35447704e-02, 3.24777366e-02, 3.04490902e-02, 3.00246758e-02,\n",
              "       2.37167400e-02, 2.08329788e-02, 1.18357869e-02, 8.88449559e-03,\n",
              "       4.55347471e-03, 7.98135931e-04, 2.67271490e-32, 3.42850975e-33])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jbg1uC2HfXC8",
        "outputId": "e156d694-4b2a-469c-c4c6-bb4dba632504"
      },
      "source": [
        "var1=np.cumsum(np.round(var,decimals=4)*100)\n",
        "var1"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([13.55, 20.41, 26.65, 31.98, 36.74, 41.42, 45.79, 50.07, 54.16,\n",
              "       58.18, 62.11, 65.94, 69.58, 73.21, 76.79, 80.29, 83.64, 86.89,\n",
              "       89.93, 92.93, 95.3 , 97.38, 98.56, 99.45, 99.91, 99.99, 99.99,\n",
              "       99.99])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "id": "fAxNj--TfXGW",
        "outputId": "0e4e97f1-9d9d-4b38-ef9c-ad08f06456d9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.plot(var1,color='red')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f67cd138350>]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1tLM9VOf0VQ"
      },
      "source": [
        "**hence here we will choose 24 pcs outoff 28 for further procedure**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "jgZmwKjnfXIV",
        "outputId": "c3689c1b-6f79-471b-b8bd-5774d81daf42"
      },
      "source": [
        "finaldf=pd.concat([pd.DataFrame(pca_values[:,0:24],columns=['pc1','pc2','pc3','pc4','pc5','pc6','pc7',\n",
        "                                                             'pc8','pc9','pc10','pc11','pc12','pc13','pc14',\n",
        "                                                             'pc15','pc16','pc17','pc18','pc19','pc20','pc21',\n",
        "                                                             'pc22','pc23','pc24']),\n",
        "                 df[['size_category']]], axis = 1)\n",
        "finaldf.size_category.replace(('large','small'),(1,0),inplace=True)\n",
        "finaldf"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pc1</th>\n",
              "      <th>pc2</th>\n",
              "      <th>pc3</th>\n",
              "      <th>pc4</th>\n",
              "      <th>pc5</th>\n",
              "      <th>pc6</th>\n",
              "      <th>pc7</th>\n",
              "      <th>pc8</th>\n",
              "      <th>pc9</th>\n",
              "      <th>pc10</th>\n",
              "      <th>pc11</th>\n",
              "      <th>pc12</th>\n",
              "      <th>pc13</th>\n",
              "      <th>pc14</th>\n",
              "      <th>pc15</th>\n",
              "      <th>pc16</th>\n",
              "      <th>pc17</th>\n",
              "      <th>pc18</th>\n",
              "      <th>pc19</th>\n",
              "      <th>pc20</th>\n",
              "      <th>pc21</th>\n",
              "      <th>pc22</th>\n",
              "      <th>pc23</th>\n",
              "      <th>pc24</th>\n",
              "      <th>size_category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.766709</td>\n",
              "      <td>-1.320255</td>\n",
              "      <td>-0.843971</td>\n",
              "      <td>-1.994738</td>\n",
              "      <td>-1.453359</td>\n",
              "      <td>0.693985</td>\n",
              "      <td>0.308104</td>\n",
              "      <td>-0.019764</td>\n",
              "      <td>0.010161</td>\n",
              "      <td>-0.437314</td>\n",
              "      <td>-0.536738</td>\n",
              "      <td>1.234550</td>\n",
              "      <td>0.276198</td>\n",
              "      <td>-0.671216</td>\n",
              "      <td>-0.529599</td>\n",
              "      <td>-0.197543</td>\n",
              "      <td>-0.021839</td>\n",
              "      <td>0.688958</td>\n",
              "      <td>0.563603</td>\n",
              "      <td>-0.439596</td>\n",
              "      <td>-0.926619</td>\n",
              "      <td>-0.405425</td>\n",
              "      <td>-0.118719</td>\n",
              "      <td>-0.017933</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.390786</td>\n",
              "      <td>0.831062</td>\n",
              "      <td>-1.101365</td>\n",
              "      <td>1.400671</td>\n",
              "      <td>2.869388</td>\n",
              "      <td>0.965898</td>\n",
              "      <td>-2.795574</td>\n",
              "      <td>0.041095</td>\n",
              "      <td>-0.548879</td>\n",
              "      <td>0.104500</td>\n",
              "      <td>-2.876498</td>\n",
              "      <td>-0.568255</td>\n",
              "      <td>2.095225</td>\n",
              "      <td>1.417634</td>\n",
              "      <td>-0.879983</td>\n",
              "      <td>-2.503167</td>\n",
              "      <td>0.499649</td>\n",
              "      <td>0.563706</td>\n",
              "      <td>-0.703319</td>\n",
              "      <td>-1.535718</td>\n",
              "      <td>-0.892995</td>\n",
              "      <td>0.836590</td>\n",
              "      <td>0.204975</td>\n",
              "      <td>0.290771</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.690416</td>\n",
              "      <td>1.177746</td>\n",
              "      <td>-1.221998</td>\n",
              "      <td>2.442038</td>\n",
              "      <td>1.090630</td>\n",
              "      <td>0.390801</td>\n",
              "      <td>-1.586675</td>\n",
              "      <td>-2.159336</td>\n",
              "      <td>-0.090580</td>\n",
              "      <td>0.260888</td>\n",
              "      <td>-3.236229</td>\n",
              "      <td>-0.601439</td>\n",
              "      <td>1.998004</td>\n",
              "      <td>1.477351</td>\n",
              "      <td>-0.946682</td>\n",
              "      <td>-2.545144</td>\n",
              "      <td>-0.658411</td>\n",
              "      <td>-0.423618</td>\n",
              "      <td>0.860550</td>\n",
              "      <td>-1.195230</td>\n",
              "      <td>-0.297870</td>\n",
              "      <td>0.743648</td>\n",
              "      <td>0.081757</td>\n",
              "      <td>0.345915</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.359951</td>\n",
              "      <td>-1.161443</td>\n",
              "      <td>0.385728</td>\n",
              "      <td>-2.118328</td>\n",
              "      <td>-1.949601</td>\n",
              "      <td>1.027664</td>\n",
              "      <td>-0.179422</td>\n",
              "      <td>-0.250227</td>\n",
              "      <td>-0.620329</td>\n",
              "      <td>-1.343189</td>\n",
              "      <td>-0.145846</td>\n",
              "      <td>1.019492</td>\n",
              "      <td>0.576990</td>\n",
              "      <td>-0.752744</td>\n",
              "      <td>0.349346</td>\n",
              "      <td>-0.040887</td>\n",
              "      <td>0.017843</td>\n",
              "      <td>0.332572</td>\n",
              "      <td>1.164745</td>\n",
              "      <td>-1.632741</td>\n",
              "      <td>-0.817618</td>\n",
              "      <td>1.523710</td>\n",
              "      <td>-0.342302</td>\n",
              "      <td>-0.378420</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.974329</td>\n",
              "      <td>-0.842626</td>\n",
              "      <td>1.327788</td>\n",
              "      <td>0.038086</td>\n",
              "      <td>-1.124763</td>\n",
              "      <td>-0.574676</td>\n",
              "      <td>-0.777155</td>\n",
              "      <td>0.303635</td>\n",
              "      <td>0.861126</td>\n",
              "      <td>-2.024719</td>\n",
              "      <td>-0.467108</td>\n",
              "      <td>1.131879</td>\n",
              "      <td>-0.137990</td>\n",
              "      <td>-0.823316</td>\n",
              "      <td>0.402298</td>\n",
              "      <td>0.844431</td>\n",
              "      <td>1.014944</td>\n",
              "      <td>-0.618231</td>\n",
              "      <td>0.822853</td>\n",
              "      <td>-1.794109</td>\n",
              "      <td>-0.723371</td>\n",
              "      <td>2.020419</td>\n",
              "      <td>-0.545591</td>\n",
              "      <td>0.161735</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>512</th>\n",
              "      <td>-0.087560</td>\n",
              "      <td>0.153964</td>\n",
              "      <td>1.241810</td>\n",
              "      <td>1.536581</td>\n",
              "      <td>0.372425</td>\n",
              "      <td>-1.133422</td>\n",
              "      <td>-0.362287</td>\n",
              "      <td>0.766946</td>\n",
              "      <td>0.818745</td>\n",
              "      <td>-0.289632</td>\n",
              "      <td>-0.933909</td>\n",
              "      <td>0.161275</td>\n",
              "      <td>-0.398215</td>\n",
              "      <td>0.197490</td>\n",
              "      <td>-0.801640</td>\n",
              "      <td>0.300522</td>\n",
              "      <td>0.513876</td>\n",
              "      <td>0.539642</td>\n",
              "      <td>-0.052958</td>\n",
              "      <td>1.898628</td>\n",
              "      <td>-1.441786</td>\n",
              "      <td>-0.821192</td>\n",
              "      <td>-1.205707</td>\n",
              "      <td>-0.698666</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>513</th>\n",
              "      <td>0.794366</td>\n",
              "      <td>-0.083966</td>\n",
              "      <td>2.670485</td>\n",
              "      <td>0.284995</td>\n",
              "      <td>0.223323</td>\n",
              "      <td>-0.904232</td>\n",
              "      <td>-0.014849</td>\n",
              "      <td>0.107226</td>\n",
              "      <td>1.340049</td>\n",
              "      <td>-0.147246</td>\n",
              "      <td>-0.652051</td>\n",
              "      <td>-0.132893</td>\n",
              "      <td>-0.518732</td>\n",
              "      <td>0.162358</td>\n",
              "      <td>-0.274733</td>\n",
              "      <td>0.342367</td>\n",
              "      <td>0.485571</td>\n",
              "      <td>0.580150</td>\n",
              "      <td>0.384984</td>\n",
              "      <td>0.086251</td>\n",
              "      <td>-0.970693</td>\n",
              "      <td>-1.353365</td>\n",
              "      <td>-1.254890</td>\n",
              "      <td>-1.212175</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>514</th>\n",
              "      <td>0.921634</td>\n",
              "      <td>-0.264543</td>\n",
              "      <td>2.719216</td>\n",
              "      <td>-0.019643</td>\n",
              "      <td>0.242195</td>\n",
              "      <td>-0.966939</td>\n",
              "      <td>-0.118080</td>\n",
              "      <td>0.123010</td>\n",
              "      <td>1.290364</td>\n",
              "      <td>-0.177553</td>\n",
              "      <td>-0.657663</td>\n",
              "      <td>-0.083060</td>\n",
              "      <td>-0.285899</td>\n",
              "      <td>0.062647</td>\n",
              "      <td>-0.494765</td>\n",
              "      <td>0.332816</td>\n",
              "      <td>0.344047</td>\n",
              "      <td>0.122409</td>\n",
              "      <td>0.313948</td>\n",
              "      <td>0.211157</td>\n",
              "      <td>-0.777731</td>\n",
              "      <td>-1.736711</td>\n",
              "      <td>-1.154127</td>\n",
              "      <td>-1.230040</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515</th>\n",
              "      <td>-1.620549</td>\n",
              "      <td>-0.978838</td>\n",
              "      <td>0.331987</td>\n",
              "      <td>1.256638</td>\n",
              "      <td>-0.408164</td>\n",
              "      <td>0.735698</td>\n",
              "      <td>0.815510</td>\n",
              "      <td>-1.398344</td>\n",
              "      <td>0.076379</td>\n",
              "      <td>-0.005814</td>\n",
              "      <td>-0.503898</td>\n",
              "      <td>0.174276</td>\n",
              "      <td>-0.163149</td>\n",
              "      <td>0.246912</td>\n",
              "      <td>-0.147679</td>\n",
              "      <td>-0.011739</td>\n",
              "      <td>-1.035533</td>\n",
              "      <td>-0.774382</td>\n",
              "      <td>-0.216315</td>\n",
              "      <td>0.515791</td>\n",
              "      <td>0.080575</td>\n",
              "      <td>-0.055548</td>\n",
              "      <td>-0.067502</td>\n",
              "      <td>-0.311027</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>516</th>\n",
              "      <td>4.075907</td>\n",
              "      <td>-0.367441</td>\n",
              "      <td>-0.247152</td>\n",
              "      <td>0.979966</td>\n",
              "      <td>6.792273</td>\n",
              "      <td>5.943666</td>\n",
              "      <td>-1.639583</td>\n",
              "      <td>8.121827</td>\n",
              "      <td>-0.627980</td>\n",
              "      <td>4.953722</td>\n",
              "      <td>-1.411962</td>\n",
              "      <td>2.986327</td>\n",
              "      <td>-2.734589</td>\n",
              "      <td>6.584205</td>\n",
              "      <td>-6.010301</td>\n",
              "      <td>10.467443</td>\n",
              "      <td>-7.333036</td>\n",
              "      <td>0.377340</td>\n",
              "      <td>8.870354</td>\n",
              "      <td>-1.074288</td>\n",
              "      <td>2.382433</td>\n",
              "      <td>1.042850</td>\n",
              "      <td>0.296436</td>\n",
              "      <td>0.125099</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>517 rows × 25 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          pc1       pc2       pc3  ...      pc23      pc24  size_category\n",
              "0    3.766709 -1.320255 -0.843971  ... -0.118719 -0.017933              0\n",
              "1    0.390786  0.831062 -1.101365  ...  0.204975  0.290771              0\n",
              "2    0.690416  1.177746 -1.221998  ...  0.081757  0.345915              0\n",
              "3    3.359951 -1.161443  0.385728  ... -0.342302 -0.378420              0\n",
              "4    2.974329 -0.842626  1.327788  ... -0.545591  0.161735              0\n",
              "..        ...       ...       ...  ...       ...       ...            ...\n",
              "512 -0.087560  0.153964  1.241810  ... -1.205707 -0.698666              1\n",
              "513  0.794366 -0.083966  2.670485  ... -1.254890 -1.212175              1\n",
              "514  0.921634 -0.264543  2.719216  ... -1.154127 -1.230040              1\n",
              "515 -1.620549 -0.978838  0.331987  ... -0.067502 -0.311027              0\n",
              "516  4.075907 -0.367441 -0.247152  ...  0.296436  0.125099              0\n",
              "\n",
              "[517 rows x 25 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7PLe_-ffXKb"
      },
      "source": [
        "#split the data into x and y\n",
        "array=finaldf.values\n",
        "x=array[:,0:24]\n",
        "y=array[:,24]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAq60kDmfXMk"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from tensorflow.keras.optimizers import SGD"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5StC27tfXPZ",
        "outputId": "8f026962-6c9d-423d-f4da-c36681931e5b"
      },
      "source": [
        "model=Sequential()\n",
        "model.add(Dense(12,input_dim=24,activation='relu'))\n",
        "model.add(Dense(8,activation='relu'))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.fit(x,y, validation_split=0.3,epochs=150,batch_size=10)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "37/37 [==============================] - 1s 7ms/step - loss: 0.5955 - accuracy: 0.7562 - val_loss: 0.7018 - val_accuracy: 0.6731\n",
            "Epoch 2/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.5675 - accuracy: 0.7562 - val_loss: 0.6938 - val_accuracy: 0.6731\n",
            "Epoch 3/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.5494 - accuracy: 0.7562 - val_loss: 0.6924 - val_accuracy: 0.6731\n",
            "Epoch 4/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.5355 - accuracy: 0.7562 - val_loss: 0.6908 - val_accuracy: 0.6731\n",
            "Epoch 5/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.5242 - accuracy: 0.7562 - val_loss: 0.6973 - val_accuracy: 0.6731\n",
            "Epoch 6/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.5141 - accuracy: 0.7562 - val_loss: 0.6950 - val_accuracy: 0.6731\n",
            "Epoch 7/150\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.5066 - accuracy: 0.7562 - val_loss: 0.7021 - val_accuracy: 0.6731\n",
            "Epoch 8/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.4992 - accuracy: 0.7562 - val_loss: 0.7060 - val_accuracy: 0.6731\n",
            "Epoch 9/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.4932 - accuracy: 0.7562 - val_loss: 0.7095 - val_accuracy: 0.6731\n",
            "Epoch 10/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.4869 - accuracy: 0.7562 - val_loss: 0.7113 - val_accuracy: 0.6731\n",
            "Epoch 11/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.4810 - accuracy: 0.7562 - val_loss: 0.7160 - val_accuracy: 0.6731\n",
            "Epoch 12/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.4746 - accuracy: 0.7562 - val_loss: 0.7209 - val_accuracy: 0.6731\n",
            "Epoch 13/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.4699 - accuracy: 0.7562 - val_loss: 0.7217 - val_accuracy: 0.6731\n",
            "Epoch 14/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.4639 - accuracy: 0.7562 - val_loss: 0.7256 - val_accuracy: 0.6731\n",
            "Epoch 15/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.4592 - accuracy: 0.7590 - val_loss: 0.7258 - val_accuracy: 0.6731\n",
            "Epoch 16/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.4539 - accuracy: 0.7590 - val_loss: 0.7337 - val_accuracy: 0.6731\n",
            "Epoch 17/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.4489 - accuracy: 0.7618 - val_loss: 0.7347 - val_accuracy: 0.6731\n",
            "Epoch 18/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.4439 - accuracy: 0.7618 - val_loss: 0.7328 - val_accuracy: 0.6731\n",
            "Epoch 19/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.4402 - accuracy: 0.7618 - val_loss: 0.7404 - val_accuracy: 0.6731\n",
            "Epoch 20/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.4347 - accuracy: 0.7645 - val_loss: 0.7408 - val_accuracy: 0.6731\n",
            "Epoch 21/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.4304 - accuracy: 0.7729 - val_loss: 0.7429 - val_accuracy: 0.6731\n",
            "Epoch 22/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.4248 - accuracy: 0.7839 - val_loss: 0.7437 - val_accuracy: 0.6731\n",
            "Epoch 23/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.4197 - accuracy: 0.7978 - val_loss: 0.7394 - val_accuracy: 0.6795\n",
            "Epoch 24/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.4155 - accuracy: 0.8006 - val_loss: 0.7481 - val_accuracy: 0.6923\n",
            "Epoch 25/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.4105 - accuracy: 0.8006 - val_loss: 0.7325 - val_accuracy: 0.7051\n",
            "Epoch 26/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.4083 - accuracy: 0.8006 - val_loss: 0.7380 - val_accuracy: 0.7051\n",
            "Epoch 27/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.4050 - accuracy: 0.8006 - val_loss: 0.7416 - val_accuracy: 0.7051\n",
            "Epoch 28/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.3996 - accuracy: 0.8033 - val_loss: 0.7509 - val_accuracy: 0.7051\n",
            "Epoch 29/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.3956 - accuracy: 0.8033 - val_loss: 0.7601 - val_accuracy: 0.7051\n",
            "Epoch 30/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.3926 - accuracy: 0.8144 - val_loss: 0.7540 - val_accuracy: 0.7051\n",
            "Epoch 31/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.3862 - accuracy: 0.8172 - val_loss: 0.7614 - val_accuracy: 0.7115\n",
            "Epoch 32/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.3826 - accuracy: 0.8172 - val_loss: 0.7589 - val_accuracy: 0.6987\n",
            "Epoch 33/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.3784 - accuracy: 0.8255 - val_loss: 0.7553 - val_accuracy: 0.6987\n",
            "Epoch 34/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.3738 - accuracy: 0.8255 - val_loss: 0.7581 - val_accuracy: 0.6987\n",
            "Epoch 35/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.3689 - accuracy: 0.8255 - val_loss: 0.7591 - val_accuracy: 0.6987\n",
            "Epoch 36/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.3649 - accuracy: 0.8283 - val_loss: 0.7601 - val_accuracy: 0.7051\n",
            "Epoch 37/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.3586 - accuracy: 0.8310 - val_loss: 0.7650 - val_accuracy: 0.7051\n",
            "Epoch 38/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.3565 - accuracy: 0.8338 - val_loss: 0.7598 - val_accuracy: 0.6987\n",
            "Epoch 39/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.3543 - accuracy: 0.8421 - val_loss: 0.7616 - val_accuracy: 0.6987\n",
            "Epoch 40/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.3488 - accuracy: 0.8421 - val_loss: 0.7677 - val_accuracy: 0.6923\n",
            "Epoch 41/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.3429 - accuracy: 0.8476 - val_loss: 0.7705 - val_accuracy: 0.6987\n",
            "Epoch 42/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.3372 - accuracy: 0.8449 - val_loss: 0.7714 - val_accuracy: 0.6987\n",
            "Epoch 43/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.3363 - accuracy: 0.8476 - val_loss: 0.7737 - val_accuracy: 0.7115\n",
            "Epoch 44/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.3306 - accuracy: 0.8560 - val_loss: 0.7700 - val_accuracy: 0.7179\n",
            "Epoch 45/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.3237 - accuracy: 0.8698 - val_loss: 0.7696 - val_accuracy: 0.7179\n",
            "Epoch 46/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.3197 - accuracy: 0.8504 - val_loss: 0.7730 - val_accuracy: 0.7179\n",
            "Epoch 47/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.3123 - accuracy: 0.8781 - val_loss: 0.7715 - val_accuracy: 0.7244\n",
            "Epoch 48/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.3110 - accuracy: 0.8698 - val_loss: 0.7758 - val_accuracy: 0.7179\n",
            "Epoch 49/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.3064 - accuracy: 0.8753 - val_loss: 0.7733 - val_accuracy: 0.7372\n",
            "Epoch 50/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.3008 - accuracy: 0.8753 - val_loss: 0.7802 - val_accuracy: 0.7372\n",
            "Epoch 51/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.2970 - accuracy: 0.8809 - val_loss: 0.7820 - val_accuracy: 0.7372\n",
            "Epoch 52/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2920 - accuracy: 0.8892 - val_loss: 0.7852 - val_accuracy: 0.7372\n",
            "Epoch 53/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2887 - accuracy: 0.8975 - val_loss: 0.7870 - val_accuracy: 0.7436\n",
            "Epoch 54/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2837 - accuracy: 0.8975 - val_loss: 0.7915 - val_accuracy: 0.7500\n",
            "Epoch 55/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2826 - accuracy: 0.8892 - val_loss: 0.7878 - val_accuracy: 0.7436\n",
            "Epoch 56/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2753 - accuracy: 0.9030 - val_loss: 0.7946 - val_accuracy: 0.7500\n",
            "Epoch 57/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2730 - accuracy: 0.8947 - val_loss: 0.7941 - val_accuracy: 0.7500\n",
            "Epoch 58/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2660 - accuracy: 0.9086 - val_loss: 0.7959 - val_accuracy: 0.7436\n",
            "Epoch 59/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.2618 - accuracy: 0.9086 - val_loss: 0.8018 - val_accuracy: 0.7436\n",
            "Epoch 60/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2606 - accuracy: 0.9086 - val_loss: 0.8047 - val_accuracy: 0.7436\n",
            "Epoch 61/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2542 - accuracy: 0.9169 - val_loss: 0.8026 - val_accuracy: 0.7436\n",
            "Epoch 62/150\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.2515 - accuracy: 0.9169 - val_loss: 0.8121 - val_accuracy: 0.7436\n",
            "Epoch 63/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2479 - accuracy: 0.9169 - val_loss: 0.8068 - val_accuracy: 0.7436\n",
            "Epoch 64/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.2433 - accuracy: 0.9197 - val_loss: 0.8177 - val_accuracy: 0.7436\n",
            "Epoch 65/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2408 - accuracy: 0.9280 - val_loss: 0.8172 - val_accuracy: 0.7308\n",
            "Epoch 66/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.2375 - accuracy: 0.9252 - val_loss: 0.8230 - val_accuracy: 0.7372\n",
            "Epoch 67/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2335 - accuracy: 0.9252 - val_loss: 0.8274 - val_accuracy: 0.7500\n",
            "Epoch 68/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.2295 - accuracy: 0.9307 - val_loss: 0.8262 - val_accuracy: 0.7564\n",
            "Epoch 69/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2261 - accuracy: 0.9224 - val_loss: 0.8294 - val_accuracy: 0.7564\n",
            "Epoch 70/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.2236 - accuracy: 0.9335 - val_loss: 0.8295 - val_accuracy: 0.7564\n",
            "Epoch 71/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2207 - accuracy: 0.9363 - val_loss: 0.8042 - val_accuracy: 0.7628\n",
            "Epoch 72/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2183 - accuracy: 0.9418 - val_loss: 0.8164 - val_accuracy: 0.7628\n",
            "Epoch 73/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2140 - accuracy: 0.9474 - val_loss: 0.8217 - val_accuracy: 0.7628\n",
            "Epoch 74/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2117 - accuracy: 0.9418 - val_loss: 0.8275 - val_accuracy: 0.7628\n",
            "Epoch 75/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2160 - accuracy: 0.9474 - val_loss: 0.8394 - val_accuracy: 0.7564\n",
            "Epoch 76/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2092 - accuracy: 0.9446 - val_loss: 0.8407 - val_accuracy: 0.7628\n",
            "Epoch 77/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.2032 - accuracy: 0.9501 - val_loss: 0.8439 - val_accuracy: 0.7628\n",
            "Epoch 78/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1980 - accuracy: 0.9529 - val_loss: 0.8425 - val_accuracy: 0.7628\n",
            "Epoch 79/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1955 - accuracy: 0.9474 - val_loss: 0.8542 - val_accuracy: 0.7628\n",
            "Epoch 80/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1932 - accuracy: 0.9474 - val_loss: 0.8523 - val_accuracy: 0.7628\n",
            "Epoch 81/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.1892 - accuracy: 0.9501 - val_loss: 0.8625 - val_accuracy: 0.7628\n",
            "Epoch 82/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1876 - accuracy: 0.9501 - val_loss: 0.8583 - val_accuracy: 0.7692\n",
            "Epoch 83/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1875 - accuracy: 0.9446 - val_loss: 0.8568 - val_accuracy: 0.7692\n",
            "Epoch 84/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1836 - accuracy: 0.9474 - val_loss: 0.8629 - val_accuracy: 0.7692\n",
            "Epoch 85/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1799 - accuracy: 0.9529 - val_loss: 0.8747 - val_accuracy: 0.7628\n",
            "Epoch 86/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1772 - accuracy: 0.9474 - val_loss: 0.8745 - val_accuracy: 0.7628\n",
            "Epoch 87/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.1790 - accuracy: 0.9557 - val_loss: 0.8582 - val_accuracy: 0.7692\n",
            "Epoch 88/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1737 - accuracy: 0.9584 - val_loss: 0.8669 - val_accuracy: 0.7564\n",
            "Epoch 89/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1703 - accuracy: 0.9584 - val_loss: 0.8742 - val_accuracy: 0.7756\n",
            "Epoch 90/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1673 - accuracy: 0.9501 - val_loss: 0.8808 - val_accuracy: 0.7628\n",
            "Epoch 91/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1640 - accuracy: 0.9557 - val_loss: 0.8825 - val_accuracy: 0.7628\n",
            "Epoch 92/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.1635 - accuracy: 0.9612 - val_loss: 0.8831 - val_accuracy: 0.7692\n",
            "Epoch 93/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.1624 - accuracy: 0.9557 - val_loss: 0.8978 - val_accuracy: 0.7628\n",
            "Epoch 94/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1587 - accuracy: 0.9584 - val_loss: 0.9061 - val_accuracy: 0.7628\n",
            "Epoch 95/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1565 - accuracy: 0.9612 - val_loss: 0.8998 - val_accuracy: 0.7692\n",
            "Epoch 96/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1741 - accuracy: 0.9557 - val_loss: 0.8905 - val_accuracy: 0.7756\n",
            "Epoch 97/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1547 - accuracy: 0.9695 - val_loss: 0.9090 - val_accuracy: 0.7821\n",
            "Epoch 98/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.1487 - accuracy: 0.9612 - val_loss: 0.9087 - val_accuracy: 0.7756\n",
            "Epoch 99/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1470 - accuracy: 0.9612 - val_loss: 0.9116 - val_accuracy: 0.7756\n",
            "Epoch 100/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1446 - accuracy: 0.9612 - val_loss: 0.9096 - val_accuracy: 0.7756\n",
            "Epoch 101/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1582 - accuracy: 0.9584 - val_loss: 0.9144 - val_accuracy: 0.7692\n",
            "Epoch 102/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1457 - accuracy: 0.9640 - val_loss: 0.9200 - val_accuracy: 0.7692\n",
            "Epoch 103/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1398 - accuracy: 0.9695 - val_loss: 0.9205 - val_accuracy: 0.7692\n",
            "Epoch 104/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1382 - accuracy: 0.9612 - val_loss: 0.9226 - val_accuracy: 0.7692\n",
            "Epoch 105/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1361 - accuracy: 0.9612 - val_loss: 0.9235 - val_accuracy: 0.7692\n",
            "Epoch 106/150\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1343 - accuracy: 0.9640 - val_loss: 0.9302 - val_accuracy: 0.7756\n",
            "Epoch 107/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1329 - accuracy: 0.9640 - val_loss: 0.9310 - val_accuracy: 0.7756\n",
            "Epoch 108/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1370 - accuracy: 0.9640 - val_loss: 0.9289 - val_accuracy: 0.7756\n",
            "Epoch 109/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1288 - accuracy: 0.9723 - val_loss: 0.9432 - val_accuracy: 0.7756\n",
            "Epoch 110/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1264 - accuracy: 0.9695 - val_loss: 0.9413 - val_accuracy: 0.7821\n",
            "Epoch 111/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1262 - accuracy: 0.9695 - val_loss: 0.9471 - val_accuracy: 0.7756\n",
            "Epoch 112/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.1246 - accuracy: 0.9695 - val_loss: 0.9398 - val_accuracy: 0.7821\n",
            "Epoch 113/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1252 - accuracy: 0.9723 - val_loss: 0.9495 - val_accuracy: 0.7821\n",
            "Epoch 114/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1215 - accuracy: 0.9723 - val_loss: 0.9548 - val_accuracy: 0.7756\n",
            "Epoch 115/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1200 - accuracy: 0.9695 - val_loss: 0.9535 - val_accuracy: 0.7821\n",
            "Epoch 116/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1196 - accuracy: 0.9751 - val_loss: 0.9661 - val_accuracy: 0.7821\n",
            "Epoch 117/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1155 - accuracy: 0.9695 - val_loss: 0.9704 - val_accuracy: 0.7692\n",
            "Epoch 118/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1153 - accuracy: 0.9695 - val_loss: 0.9769 - val_accuracy: 0.7756\n",
            "Epoch 119/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1136 - accuracy: 0.9751 - val_loss: 0.9798 - val_accuracy: 0.7692\n",
            "Epoch 120/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1186 - accuracy: 0.9640 - val_loss: 0.9746 - val_accuracy: 0.7756\n",
            "Epoch 121/150\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.1123 - accuracy: 0.9723 - val_loss: 0.9667 - val_accuracy: 0.7885\n",
            "Epoch 122/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1069 - accuracy: 0.9723 - val_loss: 0.9788 - val_accuracy: 0.7821\n",
            "Epoch 123/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1067 - accuracy: 0.9695 - val_loss: 0.9846 - val_accuracy: 0.7692\n",
            "Epoch 124/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1057 - accuracy: 0.9695 - val_loss: 0.9899 - val_accuracy: 0.7756\n",
            "Epoch 125/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1062 - accuracy: 0.9640 - val_loss: 0.9950 - val_accuracy: 0.7692\n",
            "Epoch 126/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1023 - accuracy: 0.9806 - val_loss: 0.9991 - val_accuracy: 0.7756\n",
            "Epoch 127/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1014 - accuracy: 0.9778 - val_loss: 1.0013 - val_accuracy: 0.7756\n",
            "Epoch 128/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.1007 - accuracy: 0.9751 - val_loss: 1.0013 - val_accuracy: 0.7821\n",
            "Epoch 129/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0984 - accuracy: 0.9751 - val_loss: 1.0091 - val_accuracy: 0.7756\n",
            "Epoch 130/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0981 - accuracy: 0.9778 - val_loss: 1.0178 - val_accuracy: 0.7692\n",
            "Epoch 131/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0952 - accuracy: 0.9751 - val_loss: 1.0180 - val_accuracy: 0.7692\n",
            "Epoch 132/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0957 - accuracy: 0.9751 - val_loss: 1.0250 - val_accuracy: 0.7821\n",
            "Epoch 133/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0945 - accuracy: 0.9806 - val_loss: 1.0248 - val_accuracy: 0.7692\n",
            "Epoch 134/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0942 - accuracy: 0.9778 - val_loss: 1.0314 - val_accuracy: 0.7692\n",
            "Epoch 135/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0917 - accuracy: 0.9723 - val_loss: 1.0276 - val_accuracy: 0.7756\n",
            "Epoch 136/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0883 - accuracy: 0.9861 - val_loss: 1.0374 - val_accuracy: 0.7756\n",
            "Epoch 137/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0875 - accuracy: 0.9861 - val_loss: 1.0389 - val_accuracy: 0.7821\n",
            "Epoch 138/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0872 - accuracy: 0.9778 - val_loss: 1.0366 - val_accuracy: 0.7628\n",
            "Epoch 139/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0854 - accuracy: 0.9889 - val_loss: 1.0493 - val_accuracy: 0.7821\n",
            "Epoch 140/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0845 - accuracy: 0.9834 - val_loss: 1.0512 - val_accuracy: 0.7628\n",
            "Epoch 141/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0854 - accuracy: 0.9806 - val_loss: 1.0637 - val_accuracy: 0.7756\n",
            "Epoch 142/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0830 - accuracy: 0.9861 - val_loss: 1.0703 - val_accuracy: 0.7756\n",
            "Epoch 143/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0833 - accuracy: 0.9861 - val_loss: 1.0561 - val_accuracy: 0.7628\n",
            "Epoch 144/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0802 - accuracy: 0.9834 - val_loss: 1.0690 - val_accuracy: 0.7628\n",
            "Epoch 145/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0807 - accuracy: 0.9917 - val_loss: 1.0740 - val_accuracy: 0.7756\n",
            "Epoch 146/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0804 - accuracy: 0.9806 - val_loss: 1.0837 - val_accuracy: 0.7692\n",
            "Epoch 147/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0760 - accuracy: 0.9917 - val_loss: 1.0860 - val_accuracy: 0.7628\n",
            "Epoch 148/150\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 0.0787 - accuracy: 0.9917 - val_loss: 1.0760 - val_accuracy: 0.7628\n",
            "Epoch 149/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0754 - accuracy: 0.9861 - val_loss: 1.0884 - val_accuracy: 0.7628\n",
            "Epoch 150/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.0732 - accuracy: 0.9889 - val_loss: 1.0896 - val_accuracy: 0.7628\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f67c9758150>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S60jVQlHfXRV",
        "outputId": "8d5114c2-f9e6-48f5-d7b8-dc60c0154d69"
      },
      "source": [
        "#accuracy of model\n",
        "scores=model.evaluate(x,y)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17/17 [==============================] - 0s 1ms/step - loss: 0.3761 - accuracy: 0.9246\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqvOoax2fXTm",
        "outputId": "cd064f55-7f9c-485f-9515-358f33220a4a"
      },
      "source": [
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 92.46%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qisrWRsjfXVd",
        "outputId": "76a487ce-5d01-4138-a4dc-f219172c6948"
      },
      "source": [
        "model1=Sequential()\n",
        "model1.add(Dense(12,input_dim=24,activation='sigmoid'))\n",
        "model1.add(Dense(8,activation='sigmoid'))\n",
        "model1.add(Dense(1,activation='relu'))\n",
        "model1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model1.fit(x, y, validation_split=0.3, epochs=100, batch_size=15)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "25/25 [==============================] - 1s 9ms/step - loss: 0.5934 - accuracy: 0.7479 - val_loss: 0.6584 - val_accuracy: 0.6731\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5590 - accuracy: 0.7562 - val_loss: 0.6837 - val_accuracy: 0.6731\n",
            "Epoch 3/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5511 - accuracy: 0.7562 - val_loss: 0.6645 - val_accuracy: 0.6731\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5514 - accuracy: 0.7562 - val_loss: 0.6562 - val_accuracy: 0.6731\n",
            "Epoch 5/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5446 - accuracy: 0.7562 - val_loss: 0.6795 - val_accuracy: 0.6731\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5402 - accuracy: 0.7562 - val_loss: 0.6539 - val_accuracy: 0.6731\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5400 - accuracy: 0.7562 - val_loss: 0.6590 - val_accuracy: 0.6731\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5354 - accuracy: 0.7562 - val_loss: 0.6714 - val_accuracy: 0.6731\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5334 - accuracy: 0.7562 - val_loss: 0.6605 - val_accuracy: 0.6731\n",
            "Epoch 10/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5295 - accuracy: 0.7562 - val_loss: 0.6725 - val_accuracy: 0.6731\n",
            "Epoch 11/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5305 - accuracy: 0.7562 - val_loss: 0.6917 - val_accuracy: 0.6731\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5254 - accuracy: 0.7590 - val_loss: 0.6545 - val_accuracy: 0.6731\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5242 - accuracy: 0.7562 - val_loss: 0.6853 - val_accuracy: 0.6731\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5187 - accuracy: 0.7590 - val_loss: 0.6762 - val_accuracy: 0.6731\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5166 - accuracy: 0.7590 - val_loss: 0.6995 - val_accuracy: 0.6731\n",
            "Epoch 16/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5134 - accuracy: 0.7590 - val_loss: 0.6787 - val_accuracy: 0.6795\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5109 - accuracy: 0.7590 - val_loss: 0.7185 - val_accuracy: 0.6795\n",
            "Epoch 18/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5060 - accuracy: 0.7590 - val_loss: 0.7594 - val_accuracy: 0.6795\n",
            "Epoch 19/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5024 - accuracy: 0.7590 - val_loss: 0.7595 - val_accuracy: 0.6795\n",
            "Epoch 20/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.4997 - accuracy: 0.7590 - val_loss: 0.7586 - val_accuracy: 0.6795\n",
            "Epoch 21/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.4987 - accuracy: 0.7590 - val_loss: 0.6855 - val_accuracy: 0.6859\n",
            "Epoch 22/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.4926 - accuracy: 0.7590 - val_loss: 0.7594 - val_accuracy: 0.6859\n",
            "Epoch 23/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.4884 - accuracy: 0.7590 - val_loss: 0.8732 - val_accuracy: 0.6859\n",
            "Epoch 24/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.4867 - accuracy: 0.7590 - val_loss: 0.9291 - val_accuracy: 0.6859\n",
            "Epoch 25/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.4808 - accuracy: 0.7784 - val_loss: 0.7725 - val_accuracy: 0.6859\n",
            "Epoch 26/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.4762 - accuracy: 0.7784 - val_loss: 0.9206 - val_accuracy: 0.6859\n",
            "Epoch 27/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.4774 - accuracy: 0.7867 - val_loss: 0.8343 - val_accuracy: 0.6859\n",
            "Epoch 28/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.4704 - accuracy: 0.7812 - val_loss: 0.9375 - val_accuracy: 0.6859\n",
            "Epoch 29/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.4719 - accuracy: 0.7812 - val_loss: 0.9534 - val_accuracy: 0.6859\n",
            "Epoch 30/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.4619 - accuracy: 0.7895 - val_loss: 0.8955 - val_accuracy: 0.6923\n",
            "Epoch 31/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.4617 - accuracy: 0.7922 - val_loss: 0.8915 - val_accuracy: 0.6923\n",
            "Epoch 32/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.4551 - accuracy: 0.7922 - val_loss: 1.0519 - val_accuracy: 0.6923\n",
            "Epoch 33/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.4558 - accuracy: 0.7895 - val_loss: 1.0496 - val_accuracy: 0.6923\n",
            "Epoch 34/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.4582 - accuracy: 0.8006 - val_loss: 0.8667 - val_accuracy: 0.6987\n",
            "Epoch 35/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.4508 - accuracy: 0.7950 - val_loss: 0.9794 - val_accuracy: 0.6987\n",
            "Epoch 36/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.4449 - accuracy: 0.7950 - val_loss: 1.0478 - val_accuracy: 0.6987\n",
            "Epoch 37/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.4416 - accuracy: 0.7950 - val_loss: 1.0363 - val_accuracy: 0.6987\n",
            "Epoch 38/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.4385 - accuracy: 0.7950 - val_loss: 0.9605 - val_accuracy: 0.6987\n",
            "Epoch 39/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.4362 - accuracy: 0.7950 - val_loss: 1.0326 - val_accuracy: 0.6987\n",
            "Epoch 40/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.4311 - accuracy: 0.8033 - val_loss: 1.0247 - val_accuracy: 0.6987\n",
            "Epoch 41/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.4277 - accuracy: 0.8033 - val_loss: 1.0338 - val_accuracy: 0.6987\n",
            "Epoch 42/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.4255 - accuracy: 0.8033 - val_loss: 1.0283 - val_accuracy: 0.6987\n",
            "Epoch 43/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.4226 - accuracy: 0.8033 - val_loss: 1.0265 - val_accuracy: 0.7051\n",
            "Epoch 44/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.4179 - accuracy: 0.8033 - val_loss: 1.0180 - val_accuracy: 0.7051\n",
            "Epoch 45/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.4169 - accuracy: 0.8033 - val_loss: 1.0041 - val_accuracy: 0.7051\n",
            "Epoch 46/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.4099 - accuracy: 0.8061 - val_loss: 1.0934 - val_accuracy: 0.7051\n",
            "Epoch 47/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.4064 - accuracy: 0.8033 - val_loss: 1.0115 - val_accuracy: 0.7051\n",
            "Epoch 48/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.4055 - accuracy: 0.8116 - val_loss: 0.9935 - val_accuracy: 0.7051\n",
            "Epoch 49/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.3981 - accuracy: 0.8061 - val_loss: 1.0712 - val_accuracy: 0.7051\n",
            "Epoch 50/100\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.3960 - accuracy: 0.8061 - val_loss: 1.0725 - val_accuracy: 0.7051\n",
            "Epoch 51/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.3912 - accuracy: 0.8116 - val_loss: 1.0037 - val_accuracy: 0.7051\n",
            "Epoch 52/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.3860 - accuracy: 0.8116 - val_loss: 1.0624 - val_accuracy: 0.7051\n",
            "Epoch 53/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.3845 - accuracy: 0.8116 - val_loss: 1.0588 - val_accuracy: 0.7115\n",
            "Epoch 54/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.3799 - accuracy: 0.8144 - val_loss: 0.9848 - val_accuracy: 0.7179\n",
            "Epoch 55/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.3780 - accuracy: 0.8199 - val_loss: 1.0488 - val_accuracy: 0.7179\n",
            "Epoch 56/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.3714 - accuracy: 0.8199 - val_loss: 1.0484 - val_accuracy: 0.7179\n",
            "Epoch 57/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.3677 - accuracy: 0.8199 - val_loss: 1.0502 - val_accuracy: 0.7179\n",
            "Epoch 58/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.3661 - accuracy: 0.8144 - val_loss: 1.0450 - val_accuracy: 0.7244\n",
            "Epoch 59/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.3607 - accuracy: 0.8227 - val_loss: 1.0366 - val_accuracy: 0.7372\n",
            "Epoch 60/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.3584 - accuracy: 0.8227 - val_loss: 1.0357 - val_accuracy: 0.7308\n",
            "Epoch 61/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.3523 - accuracy: 0.8227 - val_loss: 1.0266 - val_accuracy: 0.7372\n",
            "Epoch 62/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.3494 - accuracy: 0.8227 - val_loss: 1.0257 - val_accuracy: 0.7436\n",
            "Epoch 63/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.3456 - accuracy: 0.8227 - val_loss: 1.0215 - val_accuracy: 0.7564\n",
            "Epoch 64/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.3472 - accuracy: 0.8227 - val_loss: 1.0182 - val_accuracy: 0.7564\n",
            "Epoch 65/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.3453 - accuracy: 0.8393 - val_loss: 1.0148 - val_accuracy: 0.7756\n",
            "Epoch 66/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.3365 - accuracy: 0.8283 - val_loss: 1.0157 - val_accuracy: 0.7692\n",
            "Epoch 67/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.3345 - accuracy: 0.8283 - val_loss: 1.0128 - val_accuracy: 0.7692\n",
            "Epoch 68/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.3317 - accuracy: 0.8338 - val_loss: 1.0082 - val_accuracy: 0.7756\n",
            "Epoch 69/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.3276 - accuracy: 0.8283 - val_loss: 1.0055 - val_accuracy: 0.7756\n",
            "Epoch 70/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.3228 - accuracy: 0.8338 - val_loss: 0.9998 - val_accuracy: 0.7821\n",
            "Epoch 71/100\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.3205 - accuracy: 0.8338 - val_loss: 0.9984 - val_accuracy: 0.7821\n",
            "Epoch 72/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.3226 - accuracy: 0.8310 - val_loss: 0.9964 - val_accuracy: 0.7821\n",
            "Epoch 73/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.3144 - accuracy: 0.8393 - val_loss: 0.9931 - val_accuracy: 0.7821\n",
            "Epoch 74/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.3115 - accuracy: 0.8393 - val_loss: 0.9904 - val_accuracy: 0.7821\n",
            "Epoch 75/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.3088 - accuracy: 0.8393 - val_loss: 0.9880 - val_accuracy: 0.7821\n",
            "Epoch 76/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.3050 - accuracy: 0.8393 - val_loss: 0.9849 - val_accuracy: 0.7821\n",
            "Epoch 77/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.3018 - accuracy: 0.8393 - val_loss: 0.9803 - val_accuracy: 0.7821\n",
            "Epoch 78/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2978 - accuracy: 0.8393 - val_loss: 0.9771 - val_accuracy: 0.7885\n",
            "Epoch 79/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2966 - accuracy: 0.8393 - val_loss: 0.9753 - val_accuracy: 0.7949\n",
            "Epoch 80/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.3083 - accuracy: 0.8643 - val_loss: 0.9109 - val_accuracy: 0.7885\n",
            "Epoch 81/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2915 - accuracy: 0.8504 - val_loss: 0.9702 - val_accuracy: 0.7885\n",
            "Epoch 82/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2881 - accuracy: 0.8421 - val_loss: 0.9651 - val_accuracy: 0.8013\n",
            "Epoch 83/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2869 - accuracy: 0.8670 - val_loss: 0.9652 - val_accuracy: 0.7949\n",
            "Epoch 84/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2845 - accuracy: 0.8560 - val_loss: 0.9612 - val_accuracy: 0.8013\n",
            "Epoch 85/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2805 - accuracy: 0.8587 - val_loss: 0.9587 - val_accuracy: 0.8013\n",
            "Epoch 86/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2773 - accuracy: 0.8615 - val_loss: 0.9574 - val_accuracy: 0.8013\n",
            "Epoch 87/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2741 - accuracy: 0.8615 - val_loss: 0.9543 - val_accuracy: 0.8013\n",
            "Epoch 88/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2703 - accuracy: 0.8587 - val_loss: 0.9509 - val_accuracy: 0.8013\n",
            "Epoch 89/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2694 - accuracy: 0.8560 - val_loss: 0.9479 - val_accuracy: 0.8013\n",
            "Epoch 90/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2656 - accuracy: 0.8615 - val_loss: 0.9474 - val_accuracy: 0.8077\n",
            "Epoch 91/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2617 - accuracy: 0.8615 - val_loss: 0.9463 - val_accuracy: 0.8013\n",
            "Epoch 92/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2599 - accuracy: 0.8643 - val_loss: 0.9488 - val_accuracy: 0.8333\n",
            "Epoch 93/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2570 - accuracy: 0.8643 - val_loss: 0.9428 - val_accuracy: 0.8269\n",
            "Epoch 94/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2538 - accuracy: 0.8670 - val_loss: 0.9366 - val_accuracy: 0.8333\n",
            "Epoch 95/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2516 - accuracy: 0.8670 - val_loss: 0.9340 - val_accuracy: 0.8269\n",
            "Epoch 96/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2474 - accuracy: 0.8726 - val_loss: 0.9277 - val_accuracy: 0.8269\n",
            "Epoch 97/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2467 - accuracy: 0.8698 - val_loss: 0.9273 - val_accuracy: 0.8269\n",
            "Epoch 98/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2422 - accuracy: 0.8753 - val_loss: 0.9200 - val_accuracy: 0.8397\n",
            "Epoch 99/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2391 - accuracy: 0.8698 - val_loss: 0.9150 - val_accuracy: 0.8333\n",
            "Epoch 100/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.2355 - accuracy: 0.8753 - val_loss: 0.9151 - val_accuracy: 0.8333\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f67c96708d0>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_aMCom5fXXy",
        "outputId": "b41f2cfe-40f9-4c98-84cf-65879a34fcb3"
      },
      "source": [
        "#model accuracy\n",
        "scores1=model1.evaluate(x,y)\n",
        "print(\"%s: %.2f%%\" % (model1.metrics_names[1], scores1[1]*100))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17/17 [==============================] - 0s 1ms/step - loss: 0.4381 - accuracy: 0.8627\n",
            "accuracy: 86.27%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30SN79Z4fXZq",
        "outputId": "1677351c-8c1f-425d-ed95-2ce13e7a5516"
      },
      "source": [
        "model2=Sequential()\n",
        "model2.add(Dense(12,input_dim=24,activation='relu'))\n",
        "model2.add(Dense(8,activation='relu'))\n",
        "model2.add(Dense(1,activation='relu'))\n",
        "model2.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model2.fit(x,y,epochs=100, validation_split=0.3,batch_size=15)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "25/25 [==============================] - 1s 8ms/step - loss: 2.6297 - accuracy: 0.6260 - val_loss: 3.5184 - val_accuracy: 0.5897\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1.9136 - accuracy: 0.6870 - val_loss: 3.1270 - val_accuracy: 0.6090\n",
            "Epoch 3/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1.5333 - accuracy: 0.7119 - val_loss: 2.9465 - val_accuracy: 0.6090\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1.4533 - accuracy: 0.7202 - val_loss: 2.9297 - val_accuracy: 0.6090\n",
            "Epoch 5/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1.4361 - accuracy: 0.7175 - val_loss: 2.9170 - val_accuracy: 0.6026\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1.4240 - accuracy: 0.7175 - val_loss: 2.8408 - val_accuracy: 0.6026\n",
            "Epoch 7/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1.3485 - accuracy: 0.7230 - val_loss: 2.8381 - val_accuracy: 0.6026\n",
            "Epoch 8/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1.3311 - accuracy: 0.7396 - val_loss: 2.7640 - val_accuracy: 0.6218\n",
            "Epoch 9/100\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 1.2907 - accuracy: 0.7396 - val_loss: 2.7372 - val_accuracy: 0.6282\n",
            "Epoch 10/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1.2703 - accuracy: 0.7645 - val_loss: 2.7391 - val_accuracy: 0.6282\n",
            "Epoch 11/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1.2626 - accuracy: 0.7701 - val_loss: 2.7361 - val_accuracy: 0.6282\n",
            "Epoch 12/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1.2575 - accuracy: 0.7673 - val_loss: 2.7308 - val_accuracy: 0.6282\n",
            "Epoch 13/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1.2509 - accuracy: 0.7729 - val_loss: 2.7222 - val_accuracy: 0.6346\n",
            "Epoch 14/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1.2451 - accuracy: 0.7756 - val_loss: 2.7155 - val_accuracy: 0.6346\n",
            "Epoch 15/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1.2416 - accuracy: 0.7839 - val_loss: 2.6480 - val_accuracy: 0.6346\n",
            "Epoch 16/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1.2355 - accuracy: 0.7867 - val_loss: 2.5687 - val_accuracy: 0.6346\n",
            "Epoch 17/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1.2306 - accuracy: 0.7867 - val_loss: 2.5614 - val_accuracy: 0.6346\n",
            "Epoch 18/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1.1934 - accuracy: 0.7867 - val_loss: 2.5447 - val_accuracy: 0.6410\n",
            "Epoch 19/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1.1012 - accuracy: 0.7895 - val_loss: 2.1458 - val_accuracy: 0.6090\n",
            "Epoch 20/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1.0303 - accuracy: 0.7645 - val_loss: 2.1388 - val_accuracy: 0.6154\n",
            "Epoch 21/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 1.0163 - accuracy: 0.7812 - val_loss: 2.1437 - val_accuracy: 0.6410\n",
            "Epoch 22/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.9784 - accuracy: 0.7895 - val_loss: 2.0557 - val_accuracy: 0.6538\n",
            "Epoch 23/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.9739 - accuracy: 0.7950 - val_loss: 2.0990 - val_accuracy: 0.6410\n",
            "Epoch 24/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.9683 - accuracy: 0.7895 - val_loss: 2.0987 - val_accuracy: 0.6282\n",
            "Epoch 25/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.9604 - accuracy: 0.8006 - val_loss: 2.1780 - val_accuracy: 0.6474\n",
            "Epoch 26/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.9530 - accuracy: 0.8006 - val_loss: 2.1780 - val_accuracy: 0.6410\n",
            "Epoch 27/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.9479 - accuracy: 0.8033 - val_loss: 2.1820 - val_accuracy: 0.6474\n",
            "Epoch 28/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.9161 - accuracy: 0.8061 - val_loss: 2.1845 - val_accuracy: 0.6474\n",
            "Epoch 29/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.8668 - accuracy: 0.8061 - val_loss: 2.1119 - val_accuracy: 0.6474\n",
            "Epoch 30/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.8613 - accuracy: 0.8061 - val_loss: 2.1200 - val_accuracy: 0.6603\n",
            "Epoch 31/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.8541 - accuracy: 0.8116 - val_loss: 2.1722 - val_accuracy: 0.6667\n",
            "Epoch 32/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.8482 - accuracy: 0.8144 - val_loss: 2.2441 - val_accuracy: 0.6731\n",
            "Epoch 33/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.8428 - accuracy: 0.8172 - val_loss: 2.1776 - val_accuracy: 0.6603\n",
            "Epoch 34/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.8387 - accuracy: 0.8172 - val_loss: 2.1808 - val_accuracy: 0.6667\n",
            "Epoch 35/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.8356 - accuracy: 0.8144 - val_loss: 2.1737 - val_accuracy: 0.6731\n",
            "Epoch 36/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.8309 - accuracy: 0.8144 - val_loss: 2.0878 - val_accuracy: 0.6923\n",
            "Epoch 37/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.8272 - accuracy: 0.8199 - val_loss: 2.0826 - val_accuracy: 0.6923\n",
            "Epoch 38/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.8229 - accuracy: 0.8199 - val_loss: 2.0768 - val_accuracy: 0.6987\n",
            "Epoch 39/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.8181 - accuracy: 0.8338 - val_loss: 1.9958 - val_accuracy: 0.6923\n",
            "Epoch 40/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.8159 - accuracy: 0.8366 - val_loss: 1.9296 - val_accuracy: 0.6923\n",
            "Epoch 41/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.8134 - accuracy: 0.8393 - val_loss: 2.0165 - val_accuracy: 0.6923\n",
            "Epoch 42/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.8090 - accuracy: 0.8421 - val_loss: 1.9045 - val_accuracy: 0.6923\n",
            "Epoch 43/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.7147 - accuracy: 0.8476 - val_loss: 1.7502 - val_accuracy: 0.6731\n",
            "Epoch 44/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.7065 - accuracy: 0.8199 - val_loss: 1.7486 - val_accuracy: 0.6859\n",
            "Epoch 45/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.6849 - accuracy: 0.8338 - val_loss: 1.8335 - val_accuracy: 0.6603\n",
            "Epoch 46/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.6692 - accuracy: 0.8421 - val_loss: 1.8387 - val_accuracy: 0.6795\n",
            "Epoch 47/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.6564 - accuracy: 0.8393 - val_loss: 1.8415 - val_accuracy: 0.6795\n",
            "Epoch 48/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.6506 - accuracy: 0.8421 - val_loss: 1.8534 - val_accuracy: 0.6987\n",
            "Epoch 49/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.6445 - accuracy: 0.8449 - val_loss: 1.8595 - val_accuracy: 0.7051\n",
            "Epoch 50/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.6408 - accuracy: 0.8421 - val_loss: 1.8604 - val_accuracy: 0.6987\n",
            "Epoch 51/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.6386 - accuracy: 0.8449 - val_loss: 1.9299 - val_accuracy: 0.6923\n",
            "Epoch 52/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.6352 - accuracy: 0.8366 - val_loss: 1.9312 - val_accuracy: 0.7051\n",
            "Epoch 53/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.6318 - accuracy: 0.8476 - val_loss: 2.0090 - val_accuracy: 0.6987\n",
            "Epoch 54/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.6280 - accuracy: 0.8504 - val_loss: 1.9338 - val_accuracy: 0.6859\n",
            "Epoch 55/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.6251 - accuracy: 0.8532 - val_loss: 1.9344 - val_accuracy: 0.6859\n",
            "Epoch 56/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.6227 - accuracy: 0.8532 - val_loss: 1.8731 - val_accuracy: 0.6923\n",
            "Epoch 57/100\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6223 - accuracy: 0.8560 - val_loss: 1.8867 - val_accuracy: 0.6987\n",
            "Epoch 58/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.6185 - accuracy: 0.8560 - val_loss: 1.8712 - val_accuracy: 0.6859\n",
            "Epoch 59/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.6169 - accuracy: 0.8560 - val_loss: 1.8733 - val_accuracy: 0.6795\n",
            "Epoch 60/100\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 0.6139 - accuracy: 0.8587 - val_loss: 1.9335 - val_accuracy: 0.6795\n",
            "Epoch 61/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.6118 - accuracy: 0.8615 - val_loss: 1.9312 - val_accuracy: 0.6859\n",
            "Epoch 62/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.6090 - accuracy: 0.8615 - val_loss: 1.9326 - val_accuracy: 0.6859\n",
            "Epoch 63/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.6086 - accuracy: 0.8615 - val_loss: 1.9329 - val_accuracy: 0.6859\n",
            "Epoch 64/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.6051 - accuracy: 0.8643 - val_loss: 1.9268 - val_accuracy: 0.6859\n",
            "Epoch 65/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.6056 - accuracy: 0.8698 - val_loss: 1.9337 - val_accuracy: 0.6603\n",
            "Epoch 66/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.6032 - accuracy: 0.8753 - val_loss: 1.9358 - val_accuracy: 0.6667\n",
            "Epoch 67/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.6006 - accuracy: 0.8726 - val_loss: 1.9357 - val_accuracy: 0.6731\n",
            "Epoch 68/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.6005 - accuracy: 0.8698 - val_loss: 1.8521 - val_accuracy: 0.6731\n",
            "Epoch 69/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5997 - accuracy: 0.8698 - val_loss: 1.9302 - val_accuracy: 0.6538\n",
            "Epoch 70/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5993 - accuracy: 0.8753 - val_loss: 1.9321 - val_accuracy: 0.6667\n",
            "Epoch 71/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5949 - accuracy: 0.8698 - val_loss: 1.9342 - val_accuracy: 0.6603\n",
            "Epoch 72/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5925 - accuracy: 0.8726 - val_loss: 1.9501 - val_accuracy: 0.6346\n",
            "Epoch 73/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5879 - accuracy: 0.8753 - val_loss: 2.0052 - val_accuracy: 0.6474\n",
            "Epoch 74/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5860 - accuracy: 0.8781 - val_loss: 2.0088 - val_accuracy: 0.6538\n",
            "Epoch 75/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5834 - accuracy: 0.8809 - val_loss: 2.0991 - val_accuracy: 0.6667\n",
            "Epoch 76/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5836 - accuracy: 0.8753 - val_loss: 2.0252 - val_accuracy: 0.6731\n",
            "Epoch 77/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5813 - accuracy: 0.8781 - val_loss: 2.0074 - val_accuracy: 0.6538\n",
            "Epoch 78/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5780 - accuracy: 0.8781 - val_loss: 2.0074 - val_accuracy: 0.6667\n",
            "Epoch 79/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5784 - accuracy: 0.8809 - val_loss: 2.0154 - val_accuracy: 0.6731\n",
            "Epoch 80/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5763 - accuracy: 0.8809 - val_loss: 2.0073 - val_accuracy: 0.6667\n",
            "Epoch 81/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5733 - accuracy: 0.8837 - val_loss: 2.0049 - val_accuracy: 0.6667\n",
            "Epoch 82/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5729 - accuracy: 0.8837 - val_loss: 2.0040 - val_accuracy: 0.6667\n",
            "Epoch 83/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5713 - accuracy: 0.8837 - val_loss: 2.0905 - val_accuracy: 0.6667\n",
            "Epoch 84/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5685 - accuracy: 0.8864 - val_loss: 2.1549 - val_accuracy: 0.6731\n",
            "Epoch 85/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5681 - accuracy: 0.8809 - val_loss: 2.0129 - val_accuracy: 0.6731\n",
            "Epoch 86/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5652 - accuracy: 0.8837 - val_loss: 1.9692 - val_accuracy: 0.6667\n",
            "Epoch 87/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5657 - accuracy: 0.8837 - val_loss: 1.9328 - val_accuracy: 0.6667\n",
            "Epoch 88/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5627 - accuracy: 0.8864 - val_loss: 1.9308 - val_accuracy: 0.6667\n",
            "Epoch 89/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5593 - accuracy: 0.8892 - val_loss: 1.9310 - val_accuracy: 0.6667\n",
            "Epoch 90/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5598 - accuracy: 0.8947 - val_loss: 1.9197 - val_accuracy: 0.6731\n",
            "Epoch 91/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5612 - accuracy: 0.8864 - val_loss: 1.9383 - val_accuracy: 0.6410\n",
            "Epoch 92/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5592 - accuracy: 0.8892 - val_loss: 1.9213 - val_accuracy: 0.6603\n",
            "Epoch 93/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5546 - accuracy: 0.8864 - val_loss: 2.0040 - val_accuracy: 0.6603\n",
            "Epoch 94/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5694 - accuracy: 0.8920 - val_loss: 1.7803 - val_accuracy: 0.6731\n",
            "Epoch 95/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5657 - accuracy: 0.8781 - val_loss: 1.9942 - val_accuracy: 0.6603\n",
            "Epoch 96/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5546 - accuracy: 0.8864 - val_loss: 2.0025 - val_accuracy: 0.6731\n",
            "Epoch 97/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5514 - accuracy: 0.8920 - val_loss: 1.9986 - val_accuracy: 0.6731\n",
            "Epoch 98/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5487 - accuracy: 0.8947 - val_loss: 1.9962 - val_accuracy: 0.6731\n",
            "Epoch 99/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5464 - accuracy: 0.8975 - val_loss: 2.0050 - val_accuracy: 0.6731\n",
            "Epoch 100/100\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 0.5449 - accuracy: 0.8920 - val_loss: 1.9973 - val_accuracy: 0.6731\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f67c9504990>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6GoyueHfXbr",
        "outputId": "3bb5de52-0e33-4ea2-916f-7e6227a46778"
      },
      "source": [
        "#model accuracy\n",
        "scores2=model2.evaluate(x,y)\n",
        "print(\"%s: %.2f%%\" % (model2.metrics_names[1], scores2[1]*100))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17/17 [==============================] - 0s 1ms/step - loss: 0.9801 - accuracy: 0.8279\n",
            "accuracy: 82.79%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-zx5VNOfXdw",
        "outputId": "52bc4499-0b67-4080-f231-e334998fadc9"
      },
      "source": [
        "model3=Sequential()\n",
        "model3.add(Dense(12,input_dim=24,activation='relu'))\n",
        "model3.add(Dense(8,activation='relu'))\n",
        "model3.add(Dense(1,activation='relu'))\n",
        "model3.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model3.fit(x,y,epochs=150, validation_split=0.3,batch_size=10)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "37/37 [==============================] - 1s 6ms/step - loss: 2.5118 - accuracy: 0.7175 - val_loss: 2.7900 - val_accuracy: 0.6282\n",
            "Epoch 2/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 2.1316 - accuracy: 0.7202 - val_loss: 2.5453 - val_accuracy: 0.5962\n",
            "Epoch 3/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 2.0361 - accuracy: 0.7202 - val_loss: 2.5131 - val_accuracy: 0.6090\n",
            "Epoch 4/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 2.0156 - accuracy: 0.7258 - val_loss: 2.5153 - val_accuracy: 0.6346\n",
            "Epoch 5/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 2.0084 - accuracy: 0.7285 - val_loss: 2.5513 - val_accuracy: 0.6346\n",
            "Epoch 6/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1.9752 - accuracy: 0.7313 - val_loss: 2.8808 - val_accuracy: 0.6923\n",
            "Epoch 7/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 1.9560 - accuracy: 0.7452 - val_loss: 2.8100 - val_accuracy: 0.6859\n",
            "Epoch 8/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1.9496 - accuracy: 0.7452 - val_loss: 2.7311 - val_accuracy: 0.6667\n",
            "Epoch 9/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1.8559 - accuracy: 0.7424 - val_loss: 2.3968 - val_accuracy: 0.6346\n",
            "Epoch 10/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 1.7275 - accuracy: 0.7424 - val_loss: 2.3890 - val_accuracy: 0.6346\n",
            "Epoch 11/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1.7176 - accuracy: 0.7396 - val_loss: 2.4720 - val_accuracy: 0.6603\n",
            "Epoch 12/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1.7140 - accuracy: 0.7424 - val_loss: 2.5351 - val_accuracy: 0.6603\n",
            "Epoch 13/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1.7097 - accuracy: 0.7396 - val_loss: 2.5352 - val_accuracy: 0.6667\n",
            "Epoch 14/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1.7060 - accuracy: 0.7396 - val_loss: 2.5347 - val_accuracy: 0.6667\n",
            "Epoch 15/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1.7021 - accuracy: 0.7452 - val_loss: 2.5347 - val_accuracy: 0.6667\n",
            "Epoch 16/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 1.6994 - accuracy: 0.7424 - val_loss: 2.5318 - val_accuracy: 0.6667\n",
            "Epoch 17/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1.6688 - accuracy: 0.7424 - val_loss: 2.4269 - val_accuracy: 0.6731\n",
            "Epoch 18/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 1.5502 - accuracy: 0.7424 - val_loss: 2.2690 - val_accuracy: 0.6474\n",
            "Epoch 19/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1.4718 - accuracy: 0.7562 - val_loss: 2.1077 - val_accuracy: 0.6410\n",
            "Epoch 20/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 1.3948 - accuracy: 0.7590 - val_loss: 2.1782 - val_accuracy: 0.6538\n",
            "Epoch 21/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1.3826 - accuracy: 0.7590 - val_loss: 2.1232 - val_accuracy: 0.6474\n",
            "Epoch 22/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 1.3784 - accuracy: 0.7562 - val_loss: 2.1276 - val_accuracy: 0.6474\n",
            "Epoch 23/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1.3733 - accuracy: 0.7562 - val_loss: 2.1042 - val_accuracy: 0.6410\n",
            "Epoch 24/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1.3692 - accuracy: 0.7645 - val_loss: 2.1719 - val_accuracy: 0.6474\n",
            "Epoch 25/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1.3401 - accuracy: 0.7673 - val_loss: 1.9082 - val_accuracy: 0.6474\n",
            "Epoch 26/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1.3303 - accuracy: 0.7673 - val_loss: 1.9089 - val_accuracy: 0.6603\n",
            "Epoch 27/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1.3260 - accuracy: 0.7618 - val_loss: 1.9387 - val_accuracy: 0.6667\n",
            "Epoch 28/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1.3225 - accuracy: 0.7590 - val_loss: 1.9985 - val_accuracy: 0.6603\n",
            "Epoch 29/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1.2257 - accuracy: 0.7756 - val_loss: 1.8840 - val_accuracy: 0.6667\n",
            "Epoch 30/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1.2024 - accuracy: 0.7784 - val_loss: 1.8908 - val_accuracy: 0.6731\n",
            "Epoch 31/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 1.1974 - accuracy: 0.7839 - val_loss: 1.9599 - val_accuracy: 0.6731\n",
            "Epoch 32/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1.1947 - accuracy: 0.7839 - val_loss: 1.9597 - val_accuracy: 0.6731\n",
            "Epoch 33/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1.1911 - accuracy: 0.7839 - val_loss: 1.9670 - val_accuracy: 0.6731\n",
            "Epoch 34/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1.1892 - accuracy: 0.7784 - val_loss: 1.9630 - val_accuracy: 0.6731\n",
            "Epoch 35/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1.1868 - accuracy: 0.7784 - val_loss: 1.9633 - val_accuracy: 0.6731\n",
            "Epoch 36/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1.1839 - accuracy: 0.7756 - val_loss: 1.9576 - val_accuracy: 0.6795\n",
            "Epoch 37/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1.1821 - accuracy: 0.7756 - val_loss: 1.9555 - val_accuracy: 0.6795\n",
            "Epoch 38/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1.1802 - accuracy: 0.7812 - val_loss: 1.9640 - val_accuracy: 0.6795\n",
            "Epoch 39/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 1.0134 - accuracy: 0.7784 - val_loss: 1.4431 - val_accuracy: 0.6667\n",
            "Epoch 40/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.9624 - accuracy: 0.7729 - val_loss: 1.4586 - val_accuracy: 0.6923\n",
            "Epoch 41/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.9569 - accuracy: 0.7784 - val_loss: 1.5187 - val_accuracy: 0.6987\n",
            "Epoch 42/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.9518 - accuracy: 0.7895 - val_loss: 1.5887 - val_accuracy: 0.6987\n",
            "Epoch 43/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.9481 - accuracy: 0.8033 - val_loss: 1.5891 - val_accuracy: 0.6859\n",
            "Epoch 44/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.9457 - accuracy: 0.8061 - val_loss: 1.5897 - val_accuracy: 0.6923\n",
            "Epoch 45/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.9420 - accuracy: 0.8061 - val_loss: 1.5261 - val_accuracy: 0.6987\n",
            "Epoch 46/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.9400 - accuracy: 0.8033 - val_loss: 1.5919 - val_accuracy: 0.7051\n",
            "Epoch 47/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.9054 - accuracy: 0.8033 - val_loss: 1.3708 - val_accuracy: 0.6923\n",
            "Epoch 48/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.8971 - accuracy: 0.8116 - val_loss: 1.2907 - val_accuracy: 0.6731\n",
            "Epoch 49/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.8932 - accuracy: 0.8172 - val_loss: 1.3633 - val_accuracy: 0.6923\n",
            "Epoch 50/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.8898 - accuracy: 0.8144 - val_loss: 1.3753 - val_accuracy: 0.6795\n",
            "Epoch 51/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.8879 - accuracy: 0.8033 - val_loss: 1.5134 - val_accuracy: 0.6923\n",
            "Epoch 52/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.8862 - accuracy: 0.8089 - val_loss: 1.5153 - val_accuracy: 0.6923\n",
            "Epoch 53/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.8835 - accuracy: 0.8116 - val_loss: 1.5045 - val_accuracy: 0.6987\n",
            "Epoch 54/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.8810 - accuracy: 0.8116 - val_loss: 1.4410 - val_accuracy: 0.6923\n",
            "Epoch 55/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.8788 - accuracy: 0.8089 - val_loss: 1.4389 - val_accuracy: 0.6923\n",
            "Epoch 56/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.8767 - accuracy: 0.8116 - val_loss: 1.4298 - val_accuracy: 0.6859\n",
            "Epoch 57/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.8744 - accuracy: 0.8116 - val_loss: 1.4543 - val_accuracy: 0.6987\n",
            "Epoch 58/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.8721 - accuracy: 0.8116 - val_loss: 1.4367 - val_accuracy: 0.6923\n",
            "Epoch 59/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.8704 - accuracy: 0.8116 - val_loss: 1.4930 - val_accuracy: 0.7051\n",
            "Epoch 60/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.8678 - accuracy: 0.8089 - val_loss: 1.3544 - val_accuracy: 0.6987\n",
            "Epoch 61/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.8689 - accuracy: 0.8199 - val_loss: 1.3432 - val_accuracy: 0.6795\n",
            "Epoch 62/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.8637 - accuracy: 0.8144 - val_loss: 1.3430 - val_accuracy: 0.6859\n",
            "Epoch 63/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.8618 - accuracy: 0.8144 - val_loss: 1.3650 - val_accuracy: 0.6795\n",
            "Epoch 64/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.8596 - accuracy: 0.8144 - val_loss: 1.3406 - val_accuracy: 0.6731\n",
            "Epoch 65/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.8570 - accuracy: 0.8144 - val_loss: 1.3365 - val_accuracy: 0.6859\n",
            "Epoch 66/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.8554 - accuracy: 0.8199 - val_loss: 1.3337 - val_accuracy: 0.6795\n",
            "Epoch 67/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.8537 - accuracy: 0.8199 - val_loss: 1.3331 - val_accuracy: 0.6795\n",
            "Epoch 68/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.8517 - accuracy: 0.8199 - val_loss: 1.3240 - val_accuracy: 0.6731\n",
            "Epoch 69/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.8489 - accuracy: 0.8172 - val_loss: 1.3208 - val_accuracy: 0.6859\n",
            "Epoch 70/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.8475 - accuracy: 0.8227 - val_loss: 1.3201 - val_accuracy: 0.6795\n",
            "Epoch 71/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.8443 - accuracy: 0.8199 - val_loss: 1.3212 - val_accuracy: 0.6859\n",
            "Epoch 72/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.8424 - accuracy: 0.8199 - val_loss: 1.3194 - val_accuracy: 0.6859\n",
            "Epoch 73/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.8407 - accuracy: 0.8227 - val_loss: 1.3209 - val_accuracy: 0.6859\n",
            "Epoch 74/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.8370 - accuracy: 0.8227 - val_loss: 1.3186 - val_accuracy: 0.6795\n",
            "Epoch 75/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.8347 - accuracy: 0.8227 - val_loss: 1.3158 - val_accuracy: 0.6795\n",
            "Epoch 76/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.8325 - accuracy: 0.8199 - val_loss: 1.3184 - val_accuracy: 0.6731\n",
            "Epoch 77/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.8301 - accuracy: 0.8227 - val_loss: 1.3123 - val_accuracy: 0.6795\n",
            "Epoch 78/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.8267 - accuracy: 0.8255 - val_loss: 1.3155 - val_accuracy: 0.6731\n",
            "Epoch 79/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.8243 - accuracy: 0.8255 - val_loss: 1.3189 - val_accuracy: 0.6795\n",
            "Epoch 80/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.8209 - accuracy: 0.8227 - val_loss: 1.3156 - val_accuracy: 0.6859\n",
            "Epoch 81/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.8196 - accuracy: 0.8227 - val_loss: 1.2475 - val_accuracy: 0.6859\n",
            "Epoch 82/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.8154 - accuracy: 0.8310 - val_loss: 1.2451 - val_accuracy: 0.6795\n",
            "Epoch 83/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.8153 - accuracy: 0.8255 - val_loss: 1.3318 - val_accuracy: 0.6731\n",
            "Epoch 84/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.8116 - accuracy: 0.8338 - val_loss: 1.2540 - val_accuracy: 0.6859\n",
            "Epoch 85/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.8096 - accuracy: 0.8310 - val_loss: 1.2576 - val_accuracy: 0.6859\n",
            "Epoch 86/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.8043 - accuracy: 0.8338 - val_loss: 1.3079 - val_accuracy: 0.6859\n",
            "Epoch 87/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.8042 - accuracy: 0.8449 - val_loss: 1.3058 - val_accuracy: 0.6859\n",
            "Epoch 88/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.7979 - accuracy: 0.8421 - val_loss: 1.3024 - val_accuracy: 0.6859\n",
            "Epoch 89/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.7939 - accuracy: 0.8393 - val_loss: 1.2967 - val_accuracy: 0.6859\n",
            "Epoch 90/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.7915 - accuracy: 0.8393 - val_loss: 1.2917 - val_accuracy: 0.6859\n",
            "Epoch 91/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.7884 - accuracy: 0.8366 - val_loss: 1.2885 - val_accuracy: 0.6859\n",
            "Epoch 92/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.7859 - accuracy: 0.8393 - val_loss: 1.2958 - val_accuracy: 0.6923\n",
            "Epoch 93/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.7836 - accuracy: 0.8421 - val_loss: 1.3023 - val_accuracy: 0.7179\n",
            "Epoch 94/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.7792 - accuracy: 0.8504 - val_loss: 1.2973 - val_accuracy: 0.7051\n",
            "Epoch 95/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.7762 - accuracy: 0.8560 - val_loss: 1.3005 - val_accuracy: 0.7115\n",
            "Epoch 96/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.7741 - accuracy: 0.8476 - val_loss: 1.3722 - val_accuracy: 0.7179\n",
            "Epoch 97/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.7702 - accuracy: 0.8532 - val_loss: 1.3742 - val_accuracy: 0.7179\n",
            "Epoch 98/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.7683 - accuracy: 0.8560 - val_loss: 1.3805 - val_accuracy: 0.7115\n",
            "Epoch 99/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.7655 - accuracy: 0.8587 - val_loss: 1.4483 - val_accuracy: 0.7179\n",
            "Epoch 100/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.7628 - accuracy: 0.8560 - val_loss: 1.3716 - val_accuracy: 0.7244\n",
            "Epoch 101/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.7299 - accuracy: 0.8698 - val_loss: 1.5942 - val_accuracy: 0.6603\n",
            "Epoch 102/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.7303 - accuracy: 0.8698 - val_loss: 1.4735 - val_accuracy: 0.6987\n",
            "Epoch 103/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.7181 - accuracy: 0.8698 - val_loss: 1.4650 - val_accuracy: 0.7115\n",
            "Epoch 104/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.7142 - accuracy: 0.8753 - val_loss: 1.4618 - val_accuracy: 0.7179\n",
            "Epoch 105/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.7093 - accuracy: 0.8809 - val_loss: 1.4741 - val_accuracy: 0.7115\n",
            "Epoch 106/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.7054 - accuracy: 0.8781 - val_loss: 1.5369 - val_accuracy: 0.7115\n",
            "Epoch 107/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.7053 - accuracy: 0.8781 - val_loss: 1.4536 - val_accuracy: 0.6987\n",
            "Epoch 108/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.7005 - accuracy: 0.8781 - val_loss: 1.6249 - val_accuracy: 0.7051\n",
            "Epoch 109/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.6990 - accuracy: 0.8809 - val_loss: 1.5550 - val_accuracy: 0.7115\n",
            "Epoch 110/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.6967 - accuracy: 0.8837 - val_loss: 1.4784 - val_accuracy: 0.7051\n",
            "Epoch 111/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.6941 - accuracy: 0.8947 - val_loss: 1.5594 - val_accuracy: 0.7115\n",
            "Epoch 112/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.6901 - accuracy: 0.8837 - val_loss: 1.5524 - val_accuracy: 0.7115\n",
            "Epoch 113/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.6858 - accuracy: 0.8864 - val_loss: 1.5482 - val_accuracy: 0.7115\n",
            "Epoch 114/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.6852 - accuracy: 0.8864 - val_loss: 1.5733 - val_accuracy: 0.6923\n",
            "Epoch 115/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.6858 - accuracy: 0.8975 - val_loss: 1.5608 - val_accuracy: 0.7051\n",
            "Epoch 116/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.6806 - accuracy: 0.8892 - val_loss: 1.5571 - val_accuracy: 0.7115\n",
            "Epoch 117/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.6749 - accuracy: 0.8892 - val_loss: 1.5639 - val_accuracy: 0.6987\n",
            "Epoch 118/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.6726 - accuracy: 0.8892 - val_loss: 1.5693 - val_accuracy: 0.6667\n",
            "Epoch 119/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.6713 - accuracy: 0.8975 - val_loss: 1.6572 - val_accuracy: 0.6731\n",
            "Epoch 120/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.6692 - accuracy: 0.8975 - val_loss: 1.6458 - val_accuracy: 0.6923\n",
            "Epoch 121/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.6701 - accuracy: 0.8920 - val_loss: 1.7354 - val_accuracy: 0.6474\n",
            "Epoch 122/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.6661 - accuracy: 0.9003 - val_loss: 1.8046 - val_accuracy: 0.6474\n",
            "Epoch 123/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.6617 - accuracy: 0.9086 - val_loss: 1.7951 - val_accuracy: 0.6410\n",
            "Epoch 124/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.6600 - accuracy: 0.8975 - val_loss: 1.8696 - val_accuracy: 0.6731\n",
            "Epoch 125/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.6560 - accuracy: 0.8947 - val_loss: 1.8098 - val_accuracy: 0.6603\n",
            "Epoch 126/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.6531 - accuracy: 0.9030 - val_loss: 1.7928 - val_accuracy: 0.6474\n",
            "Epoch 127/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.6510 - accuracy: 0.9003 - val_loss: 1.7905 - val_accuracy: 0.6538\n",
            "Epoch 128/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.6503 - accuracy: 0.9003 - val_loss: 1.8039 - val_accuracy: 0.6346\n",
            "Epoch 129/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.6474 - accuracy: 0.8975 - val_loss: 1.8898 - val_accuracy: 0.6603\n",
            "Epoch 130/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.6453 - accuracy: 0.9030 - val_loss: 1.8744 - val_accuracy: 0.6538\n",
            "Epoch 131/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.6414 - accuracy: 0.9003 - val_loss: 1.8097 - val_accuracy: 0.6603\n",
            "Epoch 132/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.6054 - accuracy: 0.9003 - val_loss: 1.7550 - val_accuracy: 0.6538\n",
            "Epoch 133/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.6082 - accuracy: 0.9030 - val_loss: 1.6953 - val_accuracy: 0.6731\n",
            "Epoch 134/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.5968 - accuracy: 0.9030 - val_loss: 1.7124 - val_accuracy: 0.6538\n",
            "Epoch 135/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.5924 - accuracy: 0.9030 - val_loss: 1.7038 - val_accuracy: 0.6603\n",
            "Epoch 136/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.5865 - accuracy: 0.9058 - val_loss: 1.7120 - val_accuracy: 0.6474\n",
            "Epoch 137/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.5842 - accuracy: 0.9086 - val_loss: 1.7080 - val_accuracy: 0.6474\n",
            "Epoch 138/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.5841 - accuracy: 0.9058 - val_loss: 1.7446 - val_accuracy: 0.6474\n",
            "Epoch 139/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.5807 - accuracy: 0.9058 - val_loss: 1.8111 - val_accuracy: 0.6474\n",
            "Epoch 140/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.5789 - accuracy: 0.9058 - val_loss: 1.7393 - val_accuracy: 0.6538\n",
            "Epoch 141/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.5781 - accuracy: 0.9114 - val_loss: 1.7184 - val_accuracy: 0.6603\n",
            "Epoch 142/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.5753 - accuracy: 0.9169 - val_loss: 1.7769 - val_accuracy: 0.6603\n",
            "Epoch 143/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.5738 - accuracy: 0.9141 - val_loss: 1.7712 - val_accuracy: 0.6538\n",
            "Epoch 144/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.5704 - accuracy: 0.9114 - val_loss: 1.8795 - val_accuracy: 0.6603\n",
            "Epoch 145/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.5703 - accuracy: 0.9197 - val_loss: 1.8292 - val_accuracy: 0.6603\n",
            "Epoch 146/150\n",
            "37/37 [==============================] - 0s 2ms/step - loss: 0.5683 - accuracy: 0.9224 - val_loss: 1.8064 - val_accuracy: 0.6603\n",
            "Epoch 147/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.5663 - accuracy: 0.9280 - val_loss: 1.8625 - val_accuracy: 0.6603\n",
            "Epoch 148/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.5639 - accuracy: 0.9224 - val_loss: 1.8784 - val_accuracy: 0.6667\n",
            "Epoch 149/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.5640 - accuracy: 0.9280 - val_loss: 1.8595 - val_accuracy: 0.6603\n",
            "Epoch 150/150\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 0.5641 - accuracy: 0.9252 - val_loss: 1.9568 - val_accuracy: 0.6603\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f67c936c790>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENakR5IBfXgY",
        "outputId": "d1909f92-8ab0-44e2-927e-f44f76b19c6a"
      },
      "source": [
        "scores3 = model3.evaluate(x, y)\n",
        "print(\"%s: %.2f%%\" % (model3.metrics_names[1], scores3[1]*100))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17/17 [==============================] - 0s 1ms/step - loss: 0.9795 - accuracy: 0.8472\n",
            "accuracy: 84.72%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBNQ9m5pgQ2G"
      },
      "source": [
        "**hence here we can analyse that the best of all iteration is first one where accuracy of the system came as 92.65%**"
      ]
    }
  ]
}